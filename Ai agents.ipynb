{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39930d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "gemini_api_key=\"AIzaSyD-99BgMe4YOiumsWnogkx_QPQN1-9Sqv8\"\n",
    "os.environ['GOOGLE_API_KEY'] = gemini_api_key\n",
    "\n",
    "GROQ_API_KEY=\"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\"\n",
    "os.environ['GROQ_API_KEY'] = GROQ_API_KEY\n",
    "\n",
    "\n",
    "cohere_api_key=\"rhPt2ghX1NaFQlYmPYS7S3hfVRqpsFRPTFo5rYZf\"\n",
    "os.environ['cohere_api_key'] = cohere_api_key\n",
    "\n",
    "\n",
    "voyage_api=\"pa-l5w3vl8YVQWbDn958fD6q1JiUvfJ7clnK2KWmroBuKw\"\n",
    "os.environ[\"VOYAGE_API_KEY\"]=voyage_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "478a29de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Loan Agent\"\n",
    "LANGCHAIN_API_KEY = 'ls__01321d45ed594748ba1d3043c5e85106'\n",
    "os.environ['LANGCHAIN_API_KEY'] = LANGCHAIN_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7fd52dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size None\n"
     ]
    }
   ],
   "source": [
    "#from langchain.text_splitter import CharacterTextSplitter\n",
    "#from langchain_community.vectorstores import FAISS\n",
    "#from langchain_voyageai import VoyageAIEmbeddings\n",
    "#from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "#\n",
    "#embeddings = VoyageAIEmbeddings(\n",
    "#    voyage_api_key=\"pa-l5w3vl8YVQWbDn958fD6q1JiUvfJ7clnK2KWmroBuKw\", model=\"voyage-2\",truncation=True\n",
    "#)\n",
    "#loader_1=CSVLoader(file_path='loan_merge.csv')\n",
    "#documents = loader_1.load()\n",
    "#text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
    "#docs = text_splitter.split_documents(documents)\n",
    "#db = FAISS.from_documents(docs, embeddings)\n",
    "#db.save_local(\"faiss_index_loan_voyage1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f3b91bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import Chroma\n",
    "embeddings = VoyageAIEmbeddings(\n",
    "     model=\"voyage-2\",batch_size=128,truncation=True\n",
    ")\n",
    "def loan_embeing_model():  \n",
    "    new_db = FAISS.load_local(\"faiss_index_loan_voyage1\", embeddings,allow_dangerous_deserialization=True)\n",
    "    new_db=new_db.as_retriever(search_kwargs={\"k\": 1})\n",
    "    return new_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6bbc8838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import time\n",
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Data model\n",
    "class GradeConclusion(BaseModel):\n",
    "    \"\"\"Binary score for conversation to see if the conversation has been reached in conclusion or going on\n",
    "    \"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Conversation has been reached to conclusion/completed 'yes' or 'no'\")\n",
    "\n",
    "# LLM with function call \n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeConclusion)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"As a grader assessing the conversation between the user and AI, your task is to determine if the conversation contains keywords or semantic cues that signal the conclusion of the interaction, such as \"Bye.\" Grade it as relevant if such indicators are present.\n",
    "Provide a binary score of 'yes' or 'no' to indicate whether the conversation has concluded. 'yes' means the conversation has ended, and 'no' means it is still ongoing.\n",
    "            \"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\",\"User question: {conversation}\"),\n",
    "    ]\n",
    ")\n",
    "retrieval_grader_1 = grade_prompt | structured_llm_grader\n",
    "\n",
    "\n",
    "\n",
    "def names(state):\n",
    "    session_id =state['session_id']\n",
    "    name=state['name']\n",
    "    documents=loan_embeing_model().get_relevant_documents(name)\n",
    "    \n",
    "    return {\n",
    "        \"documents\":documents,\n",
    "        \"name\":name,\n",
    "        \"session_id\":session_id\n",
    "                   }\n",
    "\n",
    "\n",
    "def meta_llm(state):\n",
    "    name=state['name']\n",
    "    session_id =state['session_id']\n",
    "    documents=state['documents']\n",
    "    llm = ChatGroq(model=\"mixtral-8x7b-32768\", temperature=0)\n",
    "    system = \"\"\"You are loan agent named sandy from ABC bank that has a task to talk to a customer.Looking at his past payment history \n",
    "    Financial_circumstances ,Communication,Credit_worthiness  decide to give him extension or to give him legal notice \\n\n",
    "    You are task to have conversation with customer\n",
    "    \"\"\"\n",
    "    human=\"\"\"Here is the customer profile {customer} \\n User  query {userquery}\"\"\"\n",
    "    prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", human),\n",
    "    ]\n",
    "   \n",
    ")\n",
    "    rag_chain=prompt |llm | StrOutputParser() \n",
    "    start_time=time.time()\n",
    "    chain_with_history = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    lambda session_id: SQLChatMessageHistory(\n",
    "        session_id=session_id, connection_string=\"sqlite:///history_of_conversation.db\"\n",
    "    ),\n",
    "    input_messages_key=\"userquery\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "    \n",
    "    session_id =state['session_id']\n",
    "    generation = chain_with_history.invoke(\n",
    "            {\"userquery\":\"one more time sandy\" , \"customer\": documents},\n",
    "            config={\"configurable\": {\"session_id\":session_id }}\n",
    "        )\n",
    "    print(f\"Sql Time taken for inference {time.time() - start_time} seconds\")\n",
    "    return {\n",
    "        \"generation\":generation,\n",
    "        \n",
    "                   }\n",
    "def grade_conversation(state):\n",
    "    print(\"----CHECKING THE IF THE CONVERSATION HAS BEEN ENDED OR NOT\")\n",
    "    generation=state['generation']\n",
    "    score=retrieval_grader_1.invoke({\"conversation\": generation})\n",
    "    if score.binary_score == \"yes\":\n",
    "        print(\"--CONVERSATION CONTINUES\")\n",
    "        return \"customer_voice\"\n",
    "    else:\n",
    "        print(\"--Conversation has Ended\")\n",
    "        return \"END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c45730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Dict, TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing import List\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        keys: A dictionary where each key is a string.\n",
    "    \"\"\"\n",
    "\n",
    "    session_id:str\n",
    "    documents:List[str]\n",
    "    generation:str\n",
    "    name:str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "178dd687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35af4cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_node(\"customer_voice\", names)  \n",
    "workflow.add_node(\"ai_voice\", meta_llm)  \n",
    "\n",
    "\n",
    "workflow.set_entry_point(\"customer_voice\")\n",
    "#workflow.add_edge(\"customer_voice\", \"ai_voice\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"ai_voice\",\n",
    "    grade_conversation,\n",
    "    {\n",
    "        \"customer_voice\": \"customer_voice\",\n",
    "        \"END\": END,\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"customer_voice\", \"ai_voice\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9096a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Node 'customer_voice':\"\n",
      "'\\n---\\n'\n",
      "Sql Time taken for inference 1.5090599060058594 seconds\n",
      "\"Node 'ai_voice':\"\n",
      "'\\n---\\n'\n",
      "----CHECKING THE IF THE CONVERSATION HAS BEEN ENDED OR NOT\n",
      "--CONVERSATION CONTINUES\n",
      "\"Node 'customer_voice':\"\n",
      "'\\n---\\n'\n",
      "Sql Time taken for inference 1.3801171779632568 seconds\n",
      "\"Node 'ai_voice':\"\n",
      "'\\n---\\n'\n",
      "----CHECKING THE IF THE CONVERSATION HAS BEEN ENDED OR NOT\n",
      "--CONVERSATION CONTINUES\n",
      "\"Node 'customer_voice':\"\n",
      "'\\n---\\n'\n",
      "Sql Time taken for inference 1.838852882385254 seconds\n",
      "\"Node 'ai_voice':\"\n",
      "'\\n---\\n'\n",
      "----CHECKING THE IF THE CONVERSATION HAS BEEN ENDED OR NOT\n",
      "--CONVERSATION CONTINUES\n",
      "\"Node 'customer_voice':\"\n",
      "'\\n---\\n'\n",
      "Sql Time taken for inference 8.164835929870605 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 9\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Run\u001b[39;00m\n\u001b[0;32m      4\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjames\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msession_id\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m45\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m        \n\u001b[0;32m      7\u001b[0m     }\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m app\u001b[38;5;241m.\u001b[39mstream(inputs):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;66;03m# Node\u001b[39;00m\n\u001b[0;32m     12\u001b[0m         pprint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\Python\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:615\u001b[0m, in \u001b[0;36mPregel.transform\u001b[1;34m(self, input, config, output_keys, input_keys, **kwargs)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m    607\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    613\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    614\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]]:\n\u001b[1;32m--> 615\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[0;32m    616\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    617\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n\u001b[0;32m    618\u001b[0m         config,\n\u001b[0;32m    619\u001b[0m         output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[0;32m    620\u001b[0m         input_keys\u001b[38;5;241m=\u001b[39minput_keys,\n\u001b[0;32m    621\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    622\u001b[0m     ):\n\u001b[0;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[1;32m~\\Desktop\\Python\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1880\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[1;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1878\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1879\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1880\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mnext\u001b[39m, iterator)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1881\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[0;32m   1882\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[1;32m~\\Desktop\\Python\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:348\u001b[0m, in \u001b[0;36mPregel._transform\u001b[1;34m(self, input, run_manager, config, input_keys, output_keys, interrupt)\u001b[0m\n\u001b[0;32m    341\u001b[0m futures \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    342\u001b[0m     executor\u001b[38;5;241m.\u001b[39msubmit(proc\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m proc, \u001b[38;5;28minput\u001b[39m, config \u001b[38;5;129;01min\u001b[39;00m tasks_w_config\n\u001b[0;32m    344\u001b[0m ]\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# execute tasks, and wait for one to fail or all to finish.\u001b[39;00m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;66;03m# each task is independent from all other concurrent tasks\u001b[39;00m\n\u001b[1;32m--> 348\u001b[0m done, inflight \u001b[38;5;241m=\u001b[39m \u001b[43mconcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_when\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIRST_EXCEPTION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;66;03m# interrupt on failure or timeout\u001b[39;00m\n\u001b[0;32m    355\u001b[0m _interrupt_or_proceed(done, inflight, step)\n",
      "File \u001b[1;32m~\\Desktop\\Python\\Lib\\concurrent\\futures\\_base.py:305\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(fs, timeout, return_when)\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m DoneAndNotDoneFutures(done, not_done)\n\u001b[0;32m    303\u001b[0m     waiter \u001b[38;5;241m=\u001b[39m _create_and_install_waiters(fs, return_when)\n\u001b[1;32m--> 305\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fs:\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m f\u001b[38;5;241m.\u001b[39m_condition:\n",
      "File \u001b[1;32m~\\Desktop\\Python\\Lib\\threading.py:622\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    620\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 622\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32m~\\Desktop\\Python\\Lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Run\n",
    "inputs = {\"name\":\"james\",\n",
    "          \"session_id\":\"45\"\n",
    "       \n",
    "    }\n",
    "\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "148b6a05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
