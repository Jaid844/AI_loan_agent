{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39930d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "gemini_api_key=\"AIzaSyD-99BgMe4YOiumsWnogkx_QPQN1-9Sqv8\"\n",
    "os.environ['GOOGLE_API_KEY'] = gemini_api_key\n",
    "\n",
    "GROQ_API_KEY=\"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\"\n",
    "os.environ['GROQ_API_KEY'] = GROQ_API_KEY\n",
    "\n",
    "\n",
    "cohere_api_key=\"rhPt2ghX1NaFQlYmPYS7S3hfVRqpsFRPTFo5rYZf\"\n",
    "os.environ['cohere_api_key'] = cohere_api_key\n",
    "\n",
    "\n",
    "voyage_api=\"pa-l5w3vl8YVQWbDn958fD6q1JiUvfJ7clnK2KWmroBuKw\"\n",
    "os.environ[\"VOYAGE_API_KEY\"]=voyage_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "478a29de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Loan Agent\"\n",
    "LANGCHAIN_API_KEY = 'ls__01321d45ed594748ba1d3043c5e85106'\n",
    "os.environ['LANGCHAIN_API_KEY'] = LANGCHAIN_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7fd52dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size None\n"
     ]
    }
   ],
   "source": [
    "#from langchain.text_splitter import CharacterTextSplitter\n",
    "#from langchain_community.vectorstores import FAISS\n",
    "#from langchain_voyageai import VoyageAIEmbeddings\n",
    "#from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "#\n",
    "#embeddings = VoyageAIEmbeddings(\n",
    "#    voyage_api_key=\"pa-l5w3vl8YVQWbDn958fD6q1JiUvfJ7clnK2KWmroBuKw\", model=\"voyage-2\",truncation=True\n",
    "#)\n",
    "#loader_1=CSVLoader(file_path='loan_merge.csv')\n",
    "#documents = loader_1.load()\n",
    "#text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
    "#docs = text_splitter.split_documents(documents)\n",
    "#db = FAISS.from_documents(docs, embeddings)\n",
    "#db.save_local(\"faiss_index_loan_voyage1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f3b91bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import Chroma\n",
    "embeddings = VoyageAIEmbeddings(\n",
    "     model=\"voyage-2\",batch_size=128,truncation=True\n",
    ")\n",
    "def loan_embeing_model():  \n",
    "    new_db = FAISS.load_local(\"faiss_index_loan_voyage1\", embeddings,allow_dangerous_deserialization=True)\n",
    "    new_db=new_db.as_retriever(search_kwargs={\"k\": 1})\n",
    "    return new_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6bbc8838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import time\n",
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Data model\n",
    "class GradeConclusion(BaseModel):\n",
    "    \"\"\"Binary score for conversation to see if the conversation has been reached in conclusion or going on\n",
    "    \"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Conversation has been reached to conclusion/completed 'yes' or 'no'\")\n",
    "\n",
    "# LLM with function call \n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeConclusion)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"As a grader assessing the conversation between the user and AI, your task is to determine if the conversation contains keywords or semantic cues that signal the conclusion of the interaction, such as \"Bye.\" Grade it as relevant if such indicators are present.\n",
    "Provide a binary score of 'yes' or 'no' to indicate whether the conversation has concluded. 'yes' means the conversation has ended, and 'no' means it is still ongoing.\n",
    "            \"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\",\"User question: {conversation}\"),\n",
    "    ]\n",
    ")\n",
    "retrieval_grader_1 = grade_prompt | structured_llm_grader\n",
    "\n",
    "\n",
    "\n",
    "def names(state):\n",
    "    session_id =state['session_id']\n",
    "    name=state['name']\n",
    "    documents=loan_embeing_model().get_relevant_documents(name)\n",
    "    \n",
    "    return {\n",
    "        \"documents\":documents,\n",
    "        \"name\":name,\n",
    "        \"session_id\":session_id\n",
    "                   }\n",
    "\n",
    "\n",
    "def meta_llm(state):\n",
    "    name=state['name']\n",
    "    session_id =state['session_id']\n",
    "    documents=state['documents']\n",
    "    llm = ChatGroq(model=\"mixtral-8x7b-32768\", temperature=0)\n",
    "    system = \"\"\"You are loan agent named sandy from ABC bank that has a task to talk to a customer.Looking at his past payment history \n",
    "    Financial_circumstances ,Communication,Credit_worthiness  decide to give him extension or to give him legal notice \\n\n",
    "    You are task to have conversation with customer\n",
    "    \"\"\"\n",
    "    human=\"\"\"Here is the customer profile {customer} \\n User  query {userquery}\"\"\"\n",
    "    prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", human),\n",
    "    ]\n",
    "   \n",
    ")\n",
    "    rag_chain=prompt |llm | StrOutputParser() \n",
    "    start_time=time.time()\n",
    "    chain_with_history = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    lambda session_id: SQLChatMessageHistory(\n",
    "        session_id=session_id, connection_string=\"sqlite:///history_of_conversation.db\"\n",
    "    ),\n",
    "    input_messages_key=\"userquery\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "    \n",
    "    session_id =state['session_id']\n",
    "    generation = chain_with_history.invoke(\n",
    "            {\"userquery\":\"one more time sandy\" , \"customer\": documents},\n",
    "            config={\"configurable\": {\"session_id\":session_id }}\n",
    "        )\n",
    "    print(f\"Sql Time taken for inference {time.time() - start_time} seconds\")\n",
    "    return {\n",
    "        \"generation\":generation,\n",
    "        \n",
    "                   }\n",
    "def grade_conversation(state):\n",
    "    print(\"----CHECKING THE IF THE CONVERSATION HAS BEEN ENDED OR NOT\")\n",
    "    generation=state['generation']\n",
    "    score=retrieval_grader_1.invoke({\"conversation\": generation})\n",
    "    if score.binary_score == \"yes\":\n",
    "        print(\"--CONVERSATION CONTINUES\")\n",
    "        return \"customer_voice\"\n",
    "    else:\n",
    "        print(\"--Conversation has Ended\")\n",
    "        return \"END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c45730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Dict, TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing import List\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        keys: A dictionary where each key is a string.\n",
    "    \"\"\"\n",
    "\n",
    "    session_id:str\n",
    "    documents:List[str]\n",
    "    generation:str\n",
    "    name:str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "178dd687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35af4cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_node(\"customer_voice\", names)  \n",
    "workflow.add_node(\"ai_voice\", meta_llm)  \n",
    "\n",
    "\n",
    "workflow.set_entry_point(\"customer_voice\")\n",
    "#workflow.add_edge(\"customer_voice\", \"ai_voice\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"ai_voice\",\n",
    "    grade_conversation,\n",
    "    {\n",
    "        \"customer_voice\": \"customer_voice\",\n",
    "        \"END\": END,\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"customer_voice\", \"ai_voice\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9096a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Node 'customer_voice':\"\n",
      "'\\n---\\n'\n",
      "Sql Time taken for inference 1.5090599060058594 seconds\n",
      "\"Node 'ai_voice':\"\n",
      "'\\n---\\n'\n",
      "----CHECKING THE IF THE CONVERSATION HAS BEEN ENDED OR NOT\n",
      "--CONVERSATION CONTINUES\n",
      "\"Node 'customer_voice':\"\n",
      "'\\n---\\n'\n",
      "Sql Time taken for inference 1.3801171779632568 seconds\n",
      "\"Node 'ai_voice':\"\n",
      "'\\n---\\n'\n",
      "----CHECKING THE IF THE CONVERSATION HAS BEEN ENDED OR NOT\n",
      "--CONVERSATION CONTINUES\n",
      "\"Node 'customer_voice':\"\n",
      "'\\n---\\n'\n",
      "Sql Time taken for inference 1.838852882385254 seconds\n",
      "\"Node 'ai_voice':\"\n",
      "'\\n---\\n'\n",
      "----CHECKING THE IF THE CONVERSATION HAS BEEN ENDED OR NOT\n",
      "--CONVERSATION CONTINUES\n",
      "\"Node 'customer_voice':\"\n",
      "'\\n---\\n'\n",
      "Sql Time taken for inference 8.164835929870605 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 9\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Run\u001b[39;00m\n\u001b[0;32m      4\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjames\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msession_id\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m45\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m        \n\u001b[0;32m      7\u001b[0m     }\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m app\u001b[38;5;241m.\u001b[39mstream(inputs):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;66;03m# Node\u001b[39;00m\n\u001b[0;32m     12\u001b[0m         pprint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\Python\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:615\u001b[0m, in \u001b[0;36mPregel.transform\u001b[1;34m(self, input, config, output_keys, input_keys, **kwargs)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m    607\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    613\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    614\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]]:\n\u001b[1;32m--> 615\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[0;32m    616\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    617\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n\u001b[0;32m    618\u001b[0m         config,\n\u001b[0;32m    619\u001b[0m         output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[0;32m    620\u001b[0m         input_keys\u001b[38;5;241m=\u001b[39minput_keys,\n\u001b[0;32m    621\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    622\u001b[0m     ):\n\u001b[0;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[1;32m~\\Desktop\\Python\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1880\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[1;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1878\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1879\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1880\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mnext\u001b[39m, iterator)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1881\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[0;32m   1882\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[1;32m~\\Desktop\\Python\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:348\u001b[0m, in \u001b[0;36mPregel._transform\u001b[1;34m(self, input, run_manager, config, input_keys, output_keys, interrupt)\u001b[0m\n\u001b[0;32m    341\u001b[0m futures \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    342\u001b[0m     executor\u001b[38;5;241m.\u001b[39msubmit(proc\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m proc, \u001b[38;5;28minput\u001b[39m, config \u001b[38;5;129;01min\u001b[39;00m tasks_w_config\n\u001b[0;32m    344\u001b[0m ]\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# execute tasks, and wait for one to fail or all to finish.\u001b[39;00m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;66;03m# each task is independent from all other concurrent tasks\u001b[39;00m\n\u001b[1;32m--> 348\u001b[0m done, inflight \u001b[38;5;241m=\u001b[39m \u001b[43mconcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_when\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIRST_EXCEPTION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;66;03m# interrupt on failure or timeout\u001b[39;00m\n\u001b[0;32m    355\u001b[0m _interrupt_or_proceed(done, inflight, step)\n",
      "File \u001b[1;32m~\\Desktop\\Python\\Lib\\concurrent\\futures\\_base.py:305\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(fs, timeout, return_when)\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m DoneAndNotDoneFutures(done, not_done)\n\u001b[0;32m    303\u001b[0m     waiter \u001b[38;5;241m=\u001b[39m _create_and_install_waiters(fs, return_when)\n\u001b[1;32m--> 305\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fs:\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m f\u001b[38;5;241m.\u001b[39m_condition:\n",
      "File \u001b[1;32m~\\Desktop\\Python\\Lib\\threading.py:622\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    620\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 622\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32m~\\Desktop\\Python\\Lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Run\n",
    "inputs = {\"name\":\"james\",\n",
    "          \"session_id\":\"45\"\n",
    "       \n",
    "    }\n",
    "\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bd232793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.tools.render import render_text_description\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "rendered_tools = render_text_description([monthly_payment])\n",
    "rendered_tools\n",
    "\n",
    "system_prompt = f\"\"\"You are an assistant that has access to the following set of tools. Here are the names and descriptions for each tool:\n",
    "\n",
    "{rendered_tools}\n",
    "\n",
    "Given the user input, return the name and input of the tool to use. Return your response as a JSON blob with 'name' and 'arguments' keys.and full amount of what user\n",
    "should pau\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "model=ChatOpenAI(model='gpt-3.5-turbo')\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt), (\"user\", \"{input}\")]\n",
    ")\n",
    "chain = prompt | model | JsonOutputParser() | itemgetter(\"arguments\") | monthly_payment\n",
    "chain.invoke({\"input\": \"My name is Albert Einstein what is my new payment \"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cc114478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'monthly_payment',\n",
       "  'args': {'name': 'Albert Einstein'},\n",
       "  'id': 'call_52oQYMU0mELzrAlpqDul3DIv',\n",
       "  'output': 20.0}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.runnables import (\n",
    "    Runnable,\n",
    "    RunnableLambda,\n",
    "    RunnableMap,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "tools = [monthly_payment]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "tool_map = {tool.name: tool for tool in tools}\n",
    "\n",
    "def call_tools(msg: AIMessage) -> Runnable:\n",
    "    \"\"\"Simple sequential tool calling helper.\"\"\"\n",
    "    tool_map = {tool.name: tool for tool in tools}\n",
    "    tool_calls = msg.tool_calls.copy()\n",
    "    for tool_call in tool_calls:\n",
    "        tool_call[\"output\"] = tool_map[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "    return tool_calls\n",
    "\n",
    "system=\"\"\"You are a assistant that helps user with loan adjustment ,You will calculate the loan and tell them what is their loan\n",
    "amount\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system), (\"user\", \"{input}\")]\n",
    ")\n",
    "chain = prompt|llm_with_tools | call_tools\n",
    "chain.invoke({'input':\"My name is Albert Einstein what is my new payment\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3e8edf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import Literal\n",
    "class Grade_loan_modification(BaseModel):\n",
    "    \"\"\"Binary score for conversation to see if loan adjustment has been done or not\n",
    "    \"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Conversation has been reached to Loan modification terms 'yes' or 'no'\")\n",
    "system = \"\"\" As a grader assessing the conversation between the user and AI,your task is to determine if the conversation contains\n",
    "keyword or semantic cues that signals any loan modification verdict such 'We will be adjusting your loan terms' .Grade it as relevant if such indicators are present.\n",
    "Provide a binary score of 'yes' or 'no' to indicate whether the conversation has loan modification term in it \n",
    "'yes' means the conversation has reached to loan modification , and 'no' means it has no\n",
    "\"\"\"\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(Grade_loan_modification)\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\",\"User question: {conversation}\"),\n",
    "    ]\n",
    ")\n",
    "retrieval_grader_1 = grade_prompt | structured_llm_grader\n",
    "grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"--CONVERSATION ROUTED TO LOAN ADJUSTMENT\")\n",
    "            return \"loan adjusment chain\"\n",
    "        else:\n",
    "            print(\"--Conversation CONTINUES\")\n",
    "            return \"customer_voice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "97f04807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Grade_loan_modification(binary_score='yes')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_grader_1.invoke({\"conversation\":\"We will adjust your loan terms for your payment history\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c21a702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_core.tools import ToolException\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "@tool\n",
    "def monthly_payment(name: str) -> int:\n",
    "    \"\"\"Use full whenever you want to calculate the new monthly payment whenever full name of customer is given \"\"\"\n",
    "    try:\n",
    "        df =pd.read_csv(\"Loan_amount.csv\")\n",
    "        df.set_index('Name', inplace=True)\n",
    "        interest_rate = 0.05  #\n",
    "        monthly_payment =df.loc[name]['Monthly_Payment']\n",
    "        new_monthly_payment = monthly_payment * (1 + interest_rate ) -monthly_payment\n",
    "        df.reset_index(inplace=True)\n",
    "        return monthly_payment-new_monthly_payment\n",
    "    except Exception as e:\n",
    "        raise ToolException(\"The search tool1 is not available.\" ,e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e6d691c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_json_chat_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "# Get the prompt to use - you can modify this!\n",
    "system=\"\"\"\n",
    "You are loan agent that helps people calculate their monthly payment with the avilable tool you calculate them their annual loan \n",
    "Your final answer should include what their loan amount they will pay this month\n",
    "\n",
    "\"\"\"\n",
    "human = '''TOOLS\n",
    "------\n",
    "Assistant can ask the user to use tools to look up information that may be helpful in             answering the users original question. The tools the human can use are:\n",
    "\n",
    "{tools}\n",
    "\n",
    "RESPONSE FORMAT INSTRUCTIONS\n",
    "----------------------------\n",
    "\n",
    "When responding to me, please output a response in one of two formats:\n",
    "\n",
    "**Option 1:**\n",
    "Use this if you want the human to use a tool./\n",
    "\n",
    "Markdown code snippet formatted in the following schema:\n",
    "\n",
    "```json\n",
    "{{\n",
    "    \"action\": string, \\ The action to take. Must be one of {tool_names}\n",
    "    \"action_input\": string \\ The input to the action\n",
    "}}\n",
    "```\n",
    "\n",
    "**Option #2:**\n",
    "Use this if you want to respond directly to the human. Markdown code snippet formatted             in the following schema:\n",
    "\n",
    "```json\n",
    "{{\n",
    "    \"action\": \"Final Answer\",\n",
    "    \"action_input\": string \\ You should put what you want to return to use here\n",
    "}}\n",
    "```\n",
    "\n",
    "USER'S INPUT\n",
    "--------------------\n",
    "Here is the user's input (remember to respond with a markdown code snippet of a json             blob with a single action, and NOTHING else):\n",
    "\n",
    "{input}'''\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "        (\"human\", human),\n",
    "        MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "tools=[monthly_payment]\n",
    "agent = create_json_chat_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "364baf94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"monthly_payment\",\n",
      "    \"action_input\": \"James\"\n",
      "}\n",
      "```\u001b[0m\u001b[36;1m\u001b[1;3m316.66349999999994\u001b[0m\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"James will pay a total loan amount of $316.66 this month.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'My name is James what is total loan amount',\n",
       " 'output': 'James will pay a total loan amount of $316.66 this month.'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"My name is James what is total loan amount\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e07ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_conversation(state):\n",
    "    print(\"----CHECKING THE IF THE CONVERSATION HAS BEEN ENDED OR NOT\")\n",
    "    generation=state['generation']\n",
    "    score=retrieval_grader_1.invoke({\"conversation\": generation})\n",
    "    if score.binary_score == \"yes\":\n",
    "        print(\"--CONVERSATION CONTINUES\")\n",
    "        return \"customer_voice\"\n",
    "    else:\n",
    "        print(\"--Conversation has Ended\")\n",
    "        return \"END\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
