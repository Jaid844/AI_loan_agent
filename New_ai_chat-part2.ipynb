{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2236daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "gemini_api_key=\"AIzaSyD-99BgMe4YOiumsWnogkx_QPQN1-9Sqv8\"\n",
    "\n",
    "os.environ['GOOGLE_API_KEY'] = gemini_api_key\n",
    "\n",
    "\n",
    "GROQ_API_KEY=\"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\"\n",
    "os.environ['GROQ_API_KEY'] = GROQ_API_KEY\n",
    "\n",
    "\n",
    "cohere_api_key=\"rhPt2ghX1NaFQlYmPYS7S3hfVRqpsFRPTFo5rYZf\"\n",
    "os.environ['cohere_api_key'] = cohere_api_key\n",
    "\n",
    "\n",
    "voyage_api=\"pa-l5w3vl8YVQWbDn958fD6q1JiUvfJ7clnK2KWmroBuKw\"\n",
    "os.environ[\"VOYAGE_API_KEY\"]=voyage_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab2c2946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Loan Agent Forking Bad good\"\n",
    "LANGCHAIN_API_KEY = 'ls__01321d45ed594748ba1d3043c5e85106'\n",
    "os.environ['LANGCHAIN_API_KEY'] = LANGCHAIN_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad60ce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import Chroma\n",
    "embeddings = VoyageAIEmbeddings(\n",
    "     model=\"voyage-2\",batch_size=128,truncation=True\n",
    ")\n",
    "def loan_embeing_model():  \n",
    "    new_db = FAISS.load_local(\"faiss_index_loan_voyage1\", embeddings,allow_dangerous_deserialization=True)\n",
    "    new_db=new_db.as_retriever(search_kwargs={\"k\": 1})\n",
    "    return new_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd7b8359",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langchain_core\\_api\\beta_decorator.py:87: LangChainBetaWarning: The function `with_structured_output` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from typing import Literal\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a Users Profile to specific chain \"\"\"\n",
    "\n",
    "    datasource: Literal[\"Bad\", \"Good\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given User's profile based on credit history,which profile is Good or Bad\",\n",
    "    )\n",
    "        \n",
    "llm = ChatGroq(model=\"mixtral-8x7b-32768\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(RouteQuery)\n",
    "system = \"\"\"You are a grader assessing the profiles of customer your job is to see if the credit score of the customer are good \n",
    "or bad ,Grade 'Good' if the profile is Good ,or grade it Bad if the profile of the customer is 'bad'\n",
    "            \"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\",\"Customer Profile: {profile}\"),\n",
    "    ]\n",
    ")\n",
    "customer_profile_grader= grade_prompt | structured_llm_grader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a11e3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Note that the docstrings here are crucial, as they will be passed along\n",
    "# to the model along with the class name.\n",
    "class Add(BaseModel):\n",
    "    \"\"\"Add two integers together.\"\"\"\n",
    "\n",
    "    a: int = Field(..., description=\"First integer\")\n",
    "    b: int = Field(..., description=\"Second integer\")\n",
    "\n",
    "\n",
    "class Multiply(BaseModel):\n",
    "    \"\"\"Multiply two integers together.\"\"\"\n",
    "\n",
    "    a: int = Field(..., description=\"First integer\")\n",
    "    b: int = Field(..., description=\"Second integer\")\n",
    "\n",
    "\n",
    "tools = [Add, Multiply]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f13df3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\", temperature=0)\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68269f3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'multiply' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m messages\u001b[38;5;241m.\u001b[39mappend(ai_msg)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tool_call \u001b[38;5;129;01min\u001b[39;00m ai_msg\u001b[38;5;241m.\u001b[39mtool_calls:\n\u001b[1;32m----> 7\u001b[0m     selected_tool \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m\"\u001b[39m: add, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiply\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mmultiply\u001b[49m}[tool_call[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mlower()]\n\u001b[0;32m      8\u001b[0m     tool_output \u001b[38;5;241m=\u001b[39m selected_tool\u001b[38;5;241m.\u001b[39minvoke(tool_call[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      9\u001b[0m     messages\u001b[38;5;241m.\u001b[39mappend(ToolMessage(tool_output, tool_call_id\u001b[38;5;241m=\u001b[39mtool_call[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'multiply' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "\n",
    "messages = [HumanMessage(query)]\n",
    "ai_msg = llm_with_tools.invoke(messages)\n",
    "messages.append(ai_msg)\n",
    "for tool_call in ai_msg.tool_calls:\n",
    "    selected_tool = {\"add\": add, \"multiply\": multiply}[tool_call[\"name\"].lower()]\n",
    "    tool_output = selected_tool.invoke(tool_call[\"args\"])\n",
    "    messages.append(ToolMessage(tool_output, tool_call_id=tool_call[\"id\"]))\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3949477d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
