{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2236daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "gemini_api_key=\"AIzaSyD-99BgMe4YOiumsWnogkx_QPQN1-9Sqv8\"\n",
    "os.environ['GOOGLE_API_KEY'] = gemini_api_key\n",
    "\n",
    "GROQ_API_KEY=\"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\"\n",
    "os.environ['GROQ_API_KEY'] = GROQ_API_KEY\n",
    "\n",
    "\n",
    "cohere_api_key=\"rhPt2ghX1NaFQlYmPYS7S3hfVRqpsFRPTFo5rYZf\"\n",
    "os.environ['cohere_api_key'] = cohere_api_key\n",
    "\n",
    "\n",
    "voyage_api=\"pa-l5w3vl8YVQWbDn958fD6q1JiUvfJ7clnK2KWmroBuKw\"\n",
    "os.environ[\"VOYAGE_API_KEY\"]=voyage_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab2c2946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Loan Agent Forking Bad good\"\n",
    "LANGCHAIN_API_KEY = 'ls__01321d45ed594748ba1d3043c5e85106'\n",
    "os.environ['LANGCHAIN_API_KEY'] = LANGCHAIN_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad60ce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import Chroma\n",
    "embeddings = VoyageAIEmbeddings(\n",
    "     model=\"voyage-2\",batch_size=128,truncation=True\n",
    ")\n",
    "def loan_embeing_model():  \n",
    "    new_db = FAISS.load_local(\"faiss_index_loan_voyage1\", embeddings,allow_dangerous_deserialization=True)\n",
    "    new_db=new_db.as_retriever(search_kwargs={\"k\": 1})\n",
    "    return new_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd7b8359",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langchain_core\\_api\\beta_decorator.py:87: LangChainBetaWarning: The function `with_structured_output` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "class GradeConclusion(BaseModel):\n",
    "    \"\"\"Binary score for profile to see if the profile is good profile or the bad profile\n",
    "    \"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Profile if they are good or bad based on credit history, 'good' or 'bad'\")\n",
    "        \n",
    "llm = ChatGroq(model=\"mixtral-8x7b-32768\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeConclusion)\n",
    "system = \"\"\"You are a grader assessing the profiles of customer your job is to see if the credit score of the customer are good \n",
    "or bad ,Grade 'Good' if the profile is Good ,or grade it bad if the profile of the customer is 'bad'\n",
    "            \"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\",\"Customer Profile: {profile}\"),\n",
    "    ]\n",
    ")\n",
    "customer_profile_grader= grade_prompt | structured_llm_grader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6d9730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_profile(state):\n",
    "    print(\"----CHECKING THE IF THE PROFILE IS GOOD OR BAD\")\n",
    "    generation=state['generation']\n",
    "    score=retrieval_grader_1.invoke({\"conversation\": generation})\n",
    "    if score.binary_score == \"yes\":\n",
    "        print(\"--FORKING TO GOOD CHAIN\")\n",
    "        return \"customer_voice\"\n",
    "    else:\n",
    "        print(\"--FORKING TO POOR CHAIN\")\n",
    "        return \"END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "085ee748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatMessagePromptTemplate, PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "def customer_profile_summarizer(name):\n",
    "    #name=state['name']\n",
    "    documents=loan_embeing_model().get_relevant_documents(name)\n",
    "    llm = ChatGroq(model=\"mixtral-8x7b-32768\", temperature=0)\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\" Can you summarize the profile of the customer below ,summarize the how he is with loan payment,financial circumstance\n",
    "        ,communication ,his credit worthiness  as detail as possilbe \\n\n",
    "        Here is the context {context}\n",
    "        \n",
    "        \"\"\",\n",
    "        input_variables=[\"context\"],)\n",
    "    rag_chain=prompt |llm | StrOutputParser()\n",
    "    generation = rag_chain.invoke({\"context\": documents}) \n",
    "    return {\n",
    "        \"generation\":generation,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee5f2570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder,FewShotChatMessagePromptTemplate\n",
    "def Good_Profile_Chain(name):\n",
    "    profile=profile['generation']\n",
    "    llm = ChatGroq(model=\"llama3-8b-8192\", temperature=0)\n",
    "    example = [\n",
    "            {\n",
    "                \"Loan Agent (Sandy)\": \" Good morning/afternoon, [Customer's Name]. This is Sandy calling from ABC bank. I hope you're doing well today.?\",\n",
    "                \"Customer\": \"Good morning/afternoon. Yes, thank you, I'm doing fine. How can I assist you?\"},\n",
    "\n",
    "            {\n",
    "                \"Loan Agent (Sandy)\": \"  I'm calling today to discuss your recent loan payment. I noticed there's been a delay, which is unusual given your excellent payment history. I wanted to check in with you to ensure everything is alright on your end.\",\n",
    "                \"Customer\": \" I appreciate your concern. Unfortunately, I encountered an unexpected issue with my finances this month that caused the delay in payment.\"},\n",
    "\n",
    "            {\n",
    "                \"Loan Agent (Sandy)\": \": I understand, [Customer's Name]. Life can be unpredictable, and these things happen. Your consistent payment history hasn't gone unnoticed, and I'm here to assist you in any way I can. Would you like to discuss your situation further so we can find a suitable solution together?\",\n",
    "                \"Customer\": \" Yes, please. I would appreciate any assistance you can offer.\"},\n",
    "\n",
    "            {\n",
    "                \"Loan Agent (Sandy)\": \"Certainly. Let's review your current situation and explore some options to help you get back on track. We could consider adjusting your payment schedule, setting up a payment plan, or exploring other alternatives that best fit your circumstances. Does that sound like a good starting point for us?\",\n",
    "                \"Customer\": \" Yes, that sounds reasonable.\"},\n",
    "            {\n",
    "                \"Loan Agent (Sandy)\": \" Firstly, let's review your current financial situation together. This will help us understand the extent of the issue and determine the best course of action. Do you have a clear picture of your expenses and income for the upcoming months?.\",\n",
    "                \"Customer\": \"Yes, I have some rough estimates.\"},\n",
    "            {\n",
    "                \"Loan Agent (Sandy)\": \"Excellent. Let's start by identifying any discretionary expenses that could be reduced or eliminated temporarily to free up funds for your loan payments. Additionally, if you have any assets or savings that could be used to cover the outstanding amount, now might be the time to consider utilizing them.\",\n",
    "                \"Customer\": \" That makes sense. I'll take a closer look at my budget and see where I can make adjustments.\"},\n",
    "\n",
    "            {\n",
    "                \"Loan Agent (Sandy)\": \"Perfect. Once you've identified potential areas for savings, we can discuss restructuring your payment plan. This could involve extending the loan term, adjusting the monthly installments, or exploring alternative payment arrangements that better align with your current financial situation.\",\n",
    "                \"Customer\": \" Okay, I'll gather all the necessary information and get back to you with my proposed plan..\"},\n",
    "\n",
    "            {\n",
    "                \"Loan Agent (Sandy)\": \"That sounds like a plan. In the meantime, if you have any questions or need further assistance, don't hesitate to reach out to me. I'm here to support you every step of the way.\",\n",
    "                \"Customer\": \" Thank you so much for your help. I feel more confident about resolving this issue now.\"},\n",
    "\n",
    "            {\n",
    "                \"Loan Agent (Sandy)\": \"You're very welcome, [Customer's Name]. Remember, we're a team, and together we'll find a solution that works for you. Take your time, and when you're ready, we'll discuss your proposed plan in detail.\",\n",
    "                \"Customer\": \"  I appreciate your support. I'll be in touch soon.\"},\n",
    "\n",
    "            {\"Loan Agent (Sandy)\": \"Sounds good. Take care, and have a great day!\",\n",
    "             \"Customer\": \" You too. Goodbye\"},\n",
    "\n",
    "        ]\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"ai\", \"{Loan Agent (Sandy)}\"),\n",
    "                (\"human\", \"{Customer}\"),\n",
    "            ]\n",
    "        )\n",
    "    few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "            example_prompt=prompt,\n",
    "            examples=example,\n",
    "        )\n",
    "    system=\"\"\"\n",
    "    You are loan agent called as Sandy from ABC bank here to disscuss the loan payment this customer has a good payment history \n",
    "    ,Might have some financal issue can you ask this person ,why didnt he paid this month,\n",
    "    urge him if he can some portion of the loan this month with some discount in his loan term ,due to his good history you are rewarding him\n",
    "    \n",
    "    \"\"\"\n",
    "    human = \"\"\"Make sure you dont repeat yourself during the conversation.\n",
    "        Here is the customer profile {profile} \\n\\n Users  query {userquery}\"\"\"\n",
    "    final_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system),\n",
    "                MessagesPlaceholder(variable_name=\"history\"),\n",
    "                few_shot_prompt,\n",
    "                (\"human\", human),\n",
    "            ]\n",
    "        )\n",
    "    rag_chain = final_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35388a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder,FewShotChatMessagePromptTemplate\n",
    "def Bad_Profile_Chain(name):\n",
    "    #profile=profile['generation']\n",
    "    documents=loan_embeing_model().get_relevant_documents(name)\n",
    "    llm = ChatGroq(model=\"llama3-8b-8192\", temperature=0)  \n",
    "    system=\"\"\"\n",
    "    You are loan agent called as Sandy from ABC bank here to disscuss the loan payment this customer has a bad payment history of payments\n",
    "    ,Might have some financal issue can you ask this person ,why didnt he paid this month,\n",
    "     can he pay some amount if the conversation is not good ,give him the warning the bank might take some legal action against him\n",
    "    This is a telephonic call so make a call ,talk in a that manner in small and precise manner\n",
    "\n",
    "    \"\"\"\n",
    "    human = \"\"\"Make sure you dont repeat yourself during the conversation.\n",
    "        Here is the customer profile {profile} \\n\\n Here is the user response {userquery}\"\"\"\n",
    "    final_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system),\n",
    "                (\"human\", human),\n",
    "            ]\n",
    "        )\n",
    "    rag_chain = final_prompt | llm | StrOutputParser()\n",
    "    generation = rag_chain.invoke({\"profile\": documents,\"userquery\":\"Dont ever call me \"}) \n",
    "    return {\n",
    "        \"generation\":generation,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e063b14e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generation': \"Hello Albert, this is Sandy from ABC Bank. I'm calling to discuss your loan payment. I've reviewed your payment history, and I'm concerned about the consistent late payments and defaults. Can you tell me what's been going on and why you missed this month's payment?\"}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bad_Profile_Chain(\"albert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a47cd3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c02c010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10927a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c77480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b3c079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751f1584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cab3ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c00a8a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1a21f1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatMessagePromptTemplate, PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder,FewShotChatMessagePromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "def customer_profile_summarizer(state):\n",
    "    name=state['name']\n",
    "    documents=loan_embeing_model().get_relevant_documents(name)\n",
    "    llm = ChatGroq(model=\"mixtral-8x7b-32768\", temperature=0)\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\" Can you summarize the profile of the customer below ,summarize the how he is with loan payment,financial circumstance\n",
    "        ,communication ,his credit worthiness  as detail as possilbe \\n\n",
    "        Here is the context {context}\n",
    "        \n",
    "        \"\"\",\n",
    "        input_variables=[\"context\"],)\n",
    "    rag_chain=prompt |llm | StrOutputParser()\n",
    "    generation = rag_chain.invoke({\"context\": documents}) \n",
    "    return {\n",
    "        \"Profile\":generation,\n",
    "    }\n",
    "\n",
    "def grade_profile(state):\n",
    "    print(\"----CHECKING THE IF THE PROFILE IS GOOD OR BAD\")\n",
    "    Profile=state['Profile']\n",
    "    score=customer_profile_grader.invoke({\"profile\": Profile})\n",
    "    if score.binary_score == \"yes\":\n",
    "        print(\"--FORKING TO GOOD  PROFILE CHAIN\")\n",
    "        return \"Good_Profile_Chain\"\n",
    "    else:\n",
    "        print(\"--FORKING TO POOR  PROFILE CHAIN\")\n",
    "        return \"Bad_Profile_Chain\"\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def Good_Profile_Chain(state):\n",
    "    profile=state['Profile']\n",
    "    llm = ChatGroq(model=\"llama3-8b-8192\", temperature=0)\n",
    "    example = [\n",
    "            {\n",
    "                \"Loan Agent (Sandy)\": \" Good morning/afternoon, [Customer's Name]. This is Sandy calling from ABC bank. I hope you're doing well today.?\",\n",
    "                \"Customer\": \"Good morning/afternoon. Yes, thank you, I'm doing fine. How can I assist you?\"},\n",
    "\n",
    "            {\n",
    "                \"Loan Agent (Sandy)\": \"  I'm calling today to discuss your recent loan payment. I noticed there's been a delay, which is unusual given your excellent payment history. I wanted to check in with you to ensure everything is alright on your end.\",\n",
    "                \"Customer\": \" I appreciate your concern. Unfortunately, I encountered an unexpected issue with my finances this month that caused the delay in payment.\"},\n",
    "\n",
    "            {\n",
    "                \"Loan Agent (Sandy)\": \": I understand, [Customer's Name]. Life can be unpredictable, and these things happen. Your consistent payment history hasn't gone unnoticed, and I'm here to assist you in any way I can. Would you like to discuss your situation further so we can find a suitable solution together?\",\n",
    "                \"Customer\": \" Yes, please. I would appreciate any assistance you can offer.\"},\n",
    "\n",
    "            {\n",
    "                \"Loan Agent (Sandy)\": \"Certainly. Let's review your current situation and explore some options to help you get back on track. We could consider adjusting your payment schedule, setting up a payment plan, or exploring other alternatives that best fit your circumstances. Does that sound like a good starting point for us?\",\n",
    "                \"Customer\": \" Yes, that sounds reasonable.\"},\n",
    "            {\n",
    "                \"Loan Agent (Sandy)\": \" Firstly, let's review your current financial situation together. This will help us understand the extent of the issue and determine the best course of action. Do you have a clear picture of your expenses and income for the upcoming months?.\",\n",
    "                \"Customer\": \"Yes, I have some rough estimates.\"},\n",
    "            {\n",
    "                \"Loan Agent (Sandy)\": \"Excellent. Let's start by identifying any discretionary expenses that could be reduced or eliminated temporarily to free up funds for your loan payments. Additionally, if you have any assets or savings that could be used to cover the outstanding amount, now might be the time to consider utilizing them.\",\n",
    "                \"Customer\": \" That makes sense. I'll take a closer look at my budget and see where I can make adjustments.\"},\n",
    "\n",
    "            {\n",
    "                \"Loan Agent (Sandy)\": \"Perfect. Once you've identified potential areas for savings, we can discuss restructuring your payment plan. This could involve extending the loan term, adjusting the monthly installments, or exploring alternative payment arrangements that better align with your current financial situation.\",\n",
    "                \"Customer\": \" Okay, I'll gather all the necessary information and get back to you with my proposed plan..\"},\n",
    "\n",
    "            {\n",
    "                \"Loan Agent (Sandy)\": \"That sounds like a plan. In the meantime, if you have any questions or need further assistance, don't hesitate to reach out to me. I'm here to support you every step of the way.\",\n",
    "                \"Customer\": \" Thank you so much for your help. I feel more confident about resolving this issue now.\"},\n",
    "\n",
    "            {\n",
    "                \"Loan Agent (Sandy)\": \"You're very welcome, [Customer's Name]. Remember, we're a team, and together we'll find a solution that works for you. Take your time, and when you're ready, we'll discuss your proposed plan in detail.\",\n",
    "                \"Customer\": \"  I appreciate your support. I'll be in touch soon.\"},\n",
    "\n",
    "            {\"Loan Agent (Sandy)\": \"Sounds good. Take care, and have a great day!\",\n",
    "             \"Customer\": \" You too. Goodbye\"},\n",
    "\n",
    "        ]\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"ai\", \"{Loan Agent (Sandy)}\"),\n",
    "                (\"human\", \"{Customer}\"),\n",
    "            ]\n",
    "        )\n",
    "    few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "            example_prompt=prompt,\n",
    "            examples=example,\n",
    "        )\n",
    "    system=\"\"\"\n",
    "    You are loan agent called as Sandy from ABC bank here to disscuss the loan payment this customer has a good payment history \n",
    "    ,Might have some financal issue can you ask this person ,why didnt he paid this month,\n",
    "    urge him if he can some portion of the loan this month with some discount in his loan term ,due to his good history you are rewarding him\n",
    "    \n",
    "    \"\"\"\n",
    "    human = \"\"\"Make sure you dont repeat yourself during the conversation.\n",
    "        Here is the customer profile {profile} \\n\\nHere is the user response {userquery}\"\"\"\n",
    "    final_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system),\n",
    "                MessagesPlaceholder(variable_name=\"history\"),\n",
    "                few_shot_prompt,\n",
    "                (\"human\", human),\n",
    "            ]\n",
    "        )\n",
    "    rag_chain = final_prompt | llm | StrOutputParser()\n",
    "    generation=rag_chain.invoke({\"profile\":Profile,\"userquery\":\"Hi what is this call about\"})\n",
    "    return {\n",
    "        \"generation\":generation,\n",
    "    }\n",
    "    \n",
    "\n",
    "def Bad_Profile_Chain(state):\n",
    "    profile=state['Profile']\n",
    "    #documents=loan_embeing_model().get_relevant_documents(name)\n",
    "    llm = ChatGroq(model=\"llama3-8b-8192\", temperature=0)  \n",
    "    system=\"\"\"\n",
    "    You are loan agent called as Sandy from ABC bank here to disscuss the loan payment this customer has a bad payment history of payments\n",
    "    ,Might have some financal issue can you ask this person ,why didnt he paid this month,\n",
    "     can he pay some amount if the conversation is not good ,give him the warning the bank might take some legal action against him\n",
    "    This is a telephonic call so make a call ,talk in a that manner in small and precise manner\n",
    "\n",
    "    \"\"\"\n",
    "    human = \"\"\"Make sure you dont repeat yourself during the conversation.\n",
    "        Here is the customer profile {profile} \\n\\n Here is the user response \\n\\n ---{userquery}\"\"\"\n",
    "    final_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system),\n",
    "                (\"human\", human),\n",
    "            ]\n",
    "        )\n",
    "    rag_chain = final_prompt | llm | StrOutputParser()\n",
    "    generation = rag_chain.invoke({\"profile\": profile,\"userquery\":\"Sorry our conversation has ended ,BYE \"}) \n",
    "    return {\n",
    "        \"generation\":generation,\n",
    "    }\n",
    "def grade_conversation(state):\n",
    "        class GradeConclusion(BaseModel):\n",
    "            \"\"\"Binary score for conversation to see if the conversation has been reached in conclusion\n",
    "            typically indicates that conversation have been completed  .\"\"\"\n",
    "\n",
    "            binary_score: str = Field(description=\"Conversation has been reached to conclusion/completed 'yes' or 'no'\")\n",
    "        preamble = \"\"\"As a grader assessing the conversation between the user and AI, your task is to determine if the conversation contains keyword(s) or semantic meaning indicative of concluding the interaction, such as \"Bye.\"\n",
    "            Grade it as relevant if such indicators are present. \n",
    "            Provide a binary score of 'yes' or 'no' to indicate whether the conversation has been completed or concluded.\n",
    "            'yes' means the conversation has been ended or concluded \n",
    "            'no '  means the conversation is still going on\n",
    "            \"\"\"\n",
    "        generation = state['generation']\n",
    "        llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "        structured_llm_grader = llm.with_structured_output(GradeConclusion)\n",
    "        system = \"\"\"As a grader assessing the conversation between the user and AI, your task is to determine if the conversation contains keywords or semantic cues that signal the conclusion of the interaction, such as \"Bye.\" Grade it as relevant if such indicators are present.\n",
    "        Provide a binary score of 'yes' or 'no' to indicate whether the conversation has concluded. 'yes' means the conversation has ended, and 'no' means it is still ongoing.\n",
    "                    \"\"\"\n",
    "        grade_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system),\n",
    "                (\"human\", \"User question: {conversation}\"),\n",
    "            ]\n",
    "        )\n",
    "        retrieval_grader= grade_prompt | structured_llm_grader\n",
    "        score = retrieval_grader.invoke({\"conversation\": generation})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"--CONVERSATION ENDED\")\n",
    "            return \"END\"\n",
    "        else:\n",
    "            print(\"--Conversation CONTINUES\")\n",
    "            return \"chain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f4987dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add search\n",
    "        documents: list of documents \n",
    "    \"\"\"\n",
    "    name : str\n",
    "    generation : str\n",
    "    Profile : str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0a2e9aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"profile_summarizer\", customer_profile_summarizer)  \n",
    "workflow.add_node(\"Good_Profile_Chain\", Good_Profile_Chain) \n",
    "workflow.add_node(\"Bad_Profile_Chain\", Bad_Profile_Chain)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4942d542",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.set_entry_point(\"profile_summarizer\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"profile_summarizer\",\n",
    "    grade_profile,\n",
    "    {\n",
    "        \"Good_Profile_Chain\": \"Good_Profile_Chain\",\n",
    "        \"Bad_Profile_Chain\": \"Bad_Profile_Chain\",\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "     \"Good_Profile_Chain\",\n",
    "     grade_conversation,\n",
    "    {\n",
    "        \"chain\":\"Good_Profile_Chain\",\n",
    "        \"END\":END\n",
    "    }\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "     \"Bad_Profile_Chain\",\n",
    "     grade_conversation,\n",
    "    {\n",
    "        \"chain\":\"Bad_Profile_Chain\",\n",
    "        \"END\":END\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a5ce0273",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "429202f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Node 'profile_summarizer':\"\n",
      "'\\n---\\n'\n",
      "----CHECKING THE IF THE PROFILE IS GOOD OR BAD\n",
      "--FORKING TO POOR  PROFILE CHAIN\n",
      "\"Node 'Bad_Profile_Chain':\"\n",
      "'\\n---\\n'\n",
      "--Conversation CONTINUES\n",
      "\"Node 'Bad_Profile_Chain':\"\n",
      "'\\n---\\n'\n",
      "--Conversation CONTINUES\n",
      "\"Node 'Bad_Profile_Chain':\"\n",
      "'\\n---\\n'\n",
      "--Conversation CONTINUES\n",
      "\"Node 'Bad_Profile_Chain':\"\n",
      "'\\n---\\n'\n",
      "--Conversation CONTINUES\n",
      "\"Node 'Bad_Profile_Chain':\"\n",
      "'\\n---\\n'\n",
      "--Conversation CONTINUES\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Run\n",
    "inputs = {\"name\": \"Albert\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4075d1ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
