{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc3f7590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"rough\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "LANGCHAIN_API_KEY = 'ls__01321d45ed594748ba1d3043c5e85106'\n",
    "os.environ['LANGCHAIN_API_KEY'] = LANGCHAIN_API_KEY\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43b2e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "gemini_api_key=\"AIzaSyD-99BgMe4YOiumsWnogkx_QPQN1-9Sqv8\"\n",
    "os.environ['GOOGLE_API_KEY'] = gemini_api_key\n",
    "\n",
    "GROQ_API_KEY=\"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\"\n",
    "os.environ['GROQ_API_KEY'] = GROQ_API_KEY\n",
    "\n",
    "\n",
    "cohere_api_key=\"rhPt2ghX1NaFQlYmPYS7S3hfVRqpsFRPTFo5rYZf\"\n",
    "os.environ['cohere_api_key'] = cohere_api_key\n",
    "\n",
    "\n",
    "voyage_api=\"pa-l5w3vl8YVQWbDn958fD6q1JiUvfJ7clnK2KWmroBuKw\"\n",
    "os.environ[\"VOYAGE_API_KEY\"]=voyage_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d6f140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "\n",
    "examples = [\n",
    "    (\"Hi how are you \", \"Hi am fine ,thanks for contacting to ABC bank\"),\n",
    "    (\"I would like a loan adjustment?\", \"Thank you for considering the offer, Albert. Based on the 5% discount, the adjusted portion of your loan amount would be approximately $380. Does this sound manageable for you? Would you like to proceed with this adjustment?\"),\n",
    "    (\"Still the adjustment seem to be more steep\",\"I understand your concern, Albert. As a token of our appreciation for your good payment history, we can offer you a greater discount. How about a 10% discount on the outstanding loan amount? With this discount, the adjusted payment would be approximately $360. Would this offer be more comfortable for you?\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66481780",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"Loan agent response_albert\"\n",
    "if not client.has_dataset(dataset_name=dataset_name):\n",
    "    dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "    inputs, outputs = zip(\n",
    "        *[({\"input\": text}, {\"output\": label}) for text, label in examples]\n",
    "    )\n",
    "    client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9777e66d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8be3ccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Dict, TypedDict, Literal, Optional, Union\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "\n",
    "\n",
    "\n",
    "def update_dialog_stack(left: list[str], right: Optional[str]) -> list[str]:\n",
    "    \"\"\"Push or pop the state.\"\"\"\n",
    "    if right is None:\n",
    "        return left\n",
    "    if right == \"pop\":\n",
    "        return left[:-1]\n",
    "    return left + [right]\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        keys: A dictionary where each key is a string.\n",
    "    \"\"\"\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    name: str\n",
    "    session_id: str\n",
    "    dialog_state: Annotated[\n",
    "        list[\n",
    "            Literal[\n",
    "                \"assistant\",\n",
    "                \"update_loan\"\n",
    "            ]\n",
    "        ],\n",
    "        update_dialog_stack,\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b76d81b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "\n",
    "from langchain_core.tools import ToolException\n",
    "from langchain.tools import tool\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "\n",
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "embeddings = VoyageAIEmbeddings(\n",
    "    model=\"voyage-2\", batch_size=128, truncation=True\n",
    ")\n",
    "\n",
    "\n",
    "@tool\n",
    "def monthly_payment(name: str, rate: int) -> str:\n",
    "    \"\"\"\n",
    "            This tool will help to give new monthly payment for user\n",
    "            :param rate:  rate at which loan amount will be calculated\n",
    "            :param name:  name of the customer\n",
    "            :return: string the amount the customer will pay this month\n",
    "            \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(\"Loan_amount.csv\")\n",
    "        df.set_index('Name', inplace=True)\n",
    "        interest_rate = rate / 100  #\n",
    "        monthly_payment = df.loc[name]['Monthly_Payment']\n",
    "        new_monthly_payment = monthly_payment * (1 - interest_rate)\n",
    "        df.reset_index(inplace=True)\n",
    "        if rate == 10:\n",
    "            return f\"This will be the last {new_monthly_payment}  payment for the customer {name}\"\n",
    "        elif rate == 5:\n",
    "            return f\"The initial discounted loan amount will be {new_monthly_payment} for the customer {name}\"\n",
    "    except Exception as e:\n",
    "        raise ToolException(\"The search tool1 is not available.\", e)\n",
    "\n",
    "\n",
    "def monthly_payment_1(name: str, rate: int) -> str:\n",
    "    \"\"\"\n",
    "            This tool will help to give new monthly payment for user\n",
    "            :param rate:  rate at which loan amount will be calculated\n",
    "            :param name:  name of the customer\n",
    "            :return: string the amount the customer will pay this month\n",
    "            \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(\"Loan_amount.csv\")\n",
    "        df.set_index('Name', inplace=True)\n",
    "        interest_rate = rate / 100  #\n",
    "        monthly_payment = df.loc[name]['Monthly_Payment']\n",
    "        new_monthly_payment = monthly_payment * (1 - interest_rate)\n",
    "        df.reset_index(inplace=True)\n",
    "        if rate == 10:\n",
    "            return f\"This will be the last {new_monthly_payment}  payment for the customer {name}\"\n",
    "        elif rate == 5:\n",
    "            return f\"The initial discounted loan amount will be {new_monthly_payment} for the customer {name}\"\n",
    "    except Exception as e:\n",
    "        raise ToolException(\"The search tool1 is not available.\", e)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def loan_embedding_model() -> VectorStoreRetriever:\n",
    "    new_db = FAISS.load_local(\"faiss_index_loan_voyage1\", embeddings, allow_dangerous_deserialization=True)\n",
    "    new_db = new_db.as_retriever(search_kwargs={\"k\": 1})\n",
    "    return new_db\n",
    "\n",
    "\n",
    "def _print_event(event: dict, _printed: set, max_length=1500):\n",
    "    current_state = event.get(\"dialog_state\")\n",
    "    if current_state:\n",
    "        print(f\"Currently in: \", current_state[-1])\n",
    "    message = event.get(\"messages\")\n",
    "    if message:\n",
    "        if isinstance(message, list):\n",
    "            message = message[-1]\n",
    "        if message.id not in _printed:\n",
    "            msg_repr = message.pretty_repr(html=True)\n",
    "            if len(msg_repr) > max_length:\n",
    "                msg_repr = msg_repr[:max_length] + \" ... (truncated)\"\n",
    "            print(msg_repr)\n",
    "            _printed.add(message.id)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e7466f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Callable, Union\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langgraph.prebuilt import ToolExecutor\n",
    "from langchain_community.chat_message_histories.sql import SQLChatMessageHistory\n",
    "from langchain_core.messages import HumanMessage\n",
    "from typing import Literal\n",
    "\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langgraph.graph import END\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate, \\\n",
    "    FewShotChatMessagePromptTemplate\n",
    "from langchain_core.runnables import Runnable, RunnableConfig, RunnableLambda, ensure_config\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tools = [monthly_payment]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CompleteOrEscalate(BaseModel):\n",
    "    \"\"\"A tool to mark the current task as completed and/or to escalate control of the dialog to the main assistant,\n",
    "    who can re-route the dialog based on the user's needs.\"\"\"\n",
    "\n",
    "    reason: str\n",
    "\n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"reason\": \"User changed their mind about the current task.\",\n",
    "            },\n",
    "            \"example 2\": {\n",
    "                \"reason\": \"\"\"The calculated loan amount for this month, after applying a 5% discount, is $16.67. \n",
    "Would you like to proceed with this amount, James?\"\"\",\n",
    "            },\n",
    "            \"example 3\": {\n",
    "\n",
    "                \"reason\": \"I have fully calculated the loan amount\", }\n",
    "        }\n",
    "\n",
    "\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "\n",
    "class loan_amount_5(BaseModel):\n",
    "    rate: Literal[5]\n",
    "    name: str = Field(\n",
    "        description=\"The name of the customer \"\n",
    "    )\n",
    "    dialogue: str = Field(\n",
    "        description=\"The conversion of the customer if he agrees to pay some portion of the loan amount\"\n",
    "    )\n",
    "\n",
    "\n",
    "class loan_amount_10(BaseModel):\n",
    "    rate: Literal[10]\n",
    "    name: str = Field(\n",
    "        description=\"The name of the customer \"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "class To_Loan_tool_1(BaseModel):\n",
    "    \"\"\"\n",
    "    This function will be able to calculate the loan amount for the customer ,Initially the rate will be 5 %\n",
    "    but if the customer is felling a bit steep pay,A 10 % rate will be calculated\n",
    "\n",
    "\n",
    "      \"\"\"\n",
    "\n",
    "    rate: Union[loan_amount_5, loan_amount_10] = Field(discriminator=\"rate\")\n",
    "\n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example 1\": {\n",
    "                \"rate\": 5,\n",
    "                \"name\": \"jake\",\n",
    "                \"dialogue\": \"I would like a loan adjustment\",\n",
    "            },\n",
    "            \"example 2\": {\n",
    "                \"rate\": 10,\n",
    "                \"name\": \"shela\",\n",
    "                \"dialogue\": \"The loan adjustment is too steep for me\",\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class Assistant:\n",
    "\n",
    "    def __init__(self, runnable: Runnable):\n",
    "        self.runnable = runnable\n",
    "\n",
    "    def __call__(self, state: State, config: RunnableConfig):\n",
    "        while True:\n",
    "            result = self.runnable.invoke(state)  # the input are converted into dictionary key value pair\n",
    "\n",
    "            if not result.tool_calls and (\n",
    "                    not result.content\n",
    "                    or isinstance(result.content, list)\n",
    "                    and not result.content[0].get(\"text\")\n",
    "            ):\n",
    "                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n",
    "                state = {**state, \"messages\": messages}\n",
    "                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n",
    "                state = {**state, \"messages\": messages}\n",
    "            else:\n",
    "                break\n",
    "        return {\"messages\": result}\n",
    "\n",
    "\n",
    "class Nodes():\n",
    "    \n",
    "\n",
    "    def customer_profile_summarizer(self, state):\n",
    "        config = ensure_config()  # Fetch from the context\n",
    "        configuration = config.get(\"configurable\", {})\n",
    "        name = configuration.get(\"name\", None)\n",
    "        documents = loan_embedding_model().get_relevant_documents(name)\n",
    "        llm = ChatGroq(model=\"llama3-8b-8192\", temperature=0)\n",
    "        prompt = PromptTemplate(\n",
    "            template=\"\"\" Summarize the profile of the customer below ,summarize the how he is with loan payment,financial circumstance\n",
    "                  ,communication ,his credit worthiness  as detail as possilbe \\n\n",
    "                  Here is the context {context}\n",
    "                  \"\"\",\n",
    "            input_variables=[\"context\"], )\n",
    "        rag_chain = prompt | llm | StrOutputParser()\n",
    "        generation = rag_chain.invoke({\"context\": documents})\n",
    "        return {\n",
    "            \"profile\": generation,\n",
    "        }\n",
    "\n",
    "    def primary_assistant(self, state):\n",
    "        llm = ChatOpenAI(model='gpt-3.5-turbo')\n",
    "        messages = state['messages']\n",
    "        name = state['name']\n",
    "        system = \"\"\"\"You are loan agent called as Sandy from ABC bank here to discuss the loan payment this customer has \n",
    "                  a good payment history\n",
    "                  This is going to be a telephonic call so play along have a small conversation\n",
    "                  \n",
    "                Here is the name of the customer {name}                 \n",
    "                You are given set of example so you can reference from it\n",
    "                \"INSTRUCTIONS\n",
    "                -GREET THEM WITH HELLOW AND ASK THEM WHY DID THEY PAID THIS MONTH PAYMENT\n",
    "                -ASK THEM IF THEM WILLING TO PAY SOME PORTION OF THE LOAN AMOUNT \n",
    "                -AT FIRST INITIALLY YOU WILL GIVE THEM 5 % DISCOUNT IN THEIR OUTSTANDING LOAN AMOUNT\n",
    "                -IF THEY HESITATE FOR 5 % LOAN AMOUNT ,YOU PROVIDE THEM WITH 10 % DISCOUNT RATE BECAUSE THEY ARE GOOD \n",
    "                PAYING CUSTOMER BUT REMEMBER THIS IS THE LAST RATE THEY GET ,NOT BEYOND 10%\n",
    "\n",
    "                \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", \"{system}\"),\n",
    "                (\"human\", \"{placeholder}\"),\n",
    "            ]\n",
    "        )\n",
    "        examples = [\n",
    "            {\n",
    "                \"system\": \"Good morning/afternoon, [Customer's Name]. This is Sandy calling from ABC bank. I hope you're doing well today.\",\n",
    "                \"placeholder\": \"Good morning/afternoon. Yes, thank you, I'm doing fine. How can I assist you?\"\n",
    "            },\n",
    "            {\n",
    "                \"system\": \"I'm calling today to discuss your recent loan payment. I noticed there's been a delay, which is unusual given your excellent payment history. I wanted to check in with you to ensure everything is alright on your end.\",\n",
    "                \"placeholder\": \"I appreciate your concern. Unfortunately, I encountered an unexpected issue with my finances this month that caused the delay in payment.\"\n",
    "            },\n",
    "            {\n",
    "                \"system\": \"I understand, [Customer's Name]. Life can be unpredictable, and these things happen. Your consistent payment history hasn't gone unnoticed, and I'm here to assist you in any way I can. Would you like to discuss your situation further so we can find a suitable solution together?\",\n",
    "                \"placeholder\": \"Yes, please. I would appreciate any assistance you can offer.\"\n",
    "            },\n",
    "            {\n",
    "                \"system\": \"Certainly. Let's review your current situation and explore some options to help you get back on track. We could consider adjusting your payment schedule, setting up a payment plan, or exploring other alternatives that best fit your circumstances. Does that sound like a good starting point for us?\",\n",
    "                \"placeholder\": \"Yes, that sounds reasonable.\"\n",
    "            },\n",
    "            {\n",
    "                \"system\": \"Firstly, let's review your current financial situation together. This will help us understand the extent of the issue and determine the best course of action. Do you have a clear picture of your expenses and income for the upcoming months?\",\n",
    "                \"placeholder\": \"Yes, I have some rough estimates.\"\n",
    "            },\n",
    "            {\n",
    "                \"system\": \"Excellent. Let's start by identifying any discretionary expenses that could be reduced or eliminated temporarily to free up funds for your loan payments. Additionally, if you have any assets or savings that could be used to cover the outstanding amount, now might be the time to consider utilizing them.\",\n",
    "                \"placeholder\": \"That makes sense. I'll take a closer look at my budget and see where I can make adjustments.\"\n",
    "            },\n",
    "            {\n",
    "                \"system\": \"Perfect. Once you've identified potential areas for savings, we can discuss restructuring your payment plan. This could involve extending the loan term, adjusting the monthly installments, or exploring alternative payment arrangements that better align with your current financial situation.\",\n",
    "                \"placeholder\": \"Okay, I'll gather all the necessary information and get back to you with my proposed plan.\"\n",
    "            },\n",
    "            {\n",
    "                \"system\": \"That sounds like a plan. In the meantime, if you have any questions or need further assistance, don't hesitate to reach out to me. I'm here to support you every step of the way.\",\n",
    "                \"placeholder\": \"Thank you so much for your help. I feel more confident about resolving this issue now.\"\n",
    "            },\n",
    "            {\n",
    "                \"system\": \"You're very welcome, [Customer's Name]. Remember, we're a team, and together we'll find a solution that works for you. Take your time, and when you're ready, we'll discuss your proposed plan in detail.\",\n",
    "                \"placeholder\": \"I appreciate your support. I'll be in touch soon.\"\n",
    "            },\n",
    "            {\n",
    "                \"system\": \"Before we end this call, I'd like to offer an additional option that might help. Given your excellent payment history, we can offer you a 10% discount on your monthly payments. This would reduce your payment from $400 to $360.\",\n",
    "                \"placeholder\": \"Really? That would be incredibly helpful. Thank you!\"\n",
    "            },\n",
    "            {\n",
    "                \"system\": \"You're welcome, [Customer's Name]. We'll send you the updated payment schedule and the details of your new monthly payment terms. If you have any further questions, feel free to contact me.\",\n",
    "                \"placeholder\": \"Thank you, Sandy. I'll look out for the updated information.\"\n",
    "            },\n",
    "            {\n",
    "                \"system\": \"My pleasure. Have a great day, [Customer's Name].\",\n",
    "                \"placeholder\": \"You too. Goodbye.\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "            input_variables=[\"messages\"],\n",
    "            examples=examples,\n",
    "            example_prompt=prompt\n",
    "        )\n",
    "        primary_assistant_prompt = ChatPromptTemplate.from_messages(\n",
    "            [(\"system\", system),\n",
    "             few_shot_prompt,\n",
    "             (\"placeholder\", \"{messages}\")]\n",
    "        )\n",
    "        # llm = ChatGroq(model=\"llama3-70b-8192\", temperature=0)\n",
    "        primary_assistant_runnable = primary_assistant_prompt | llm.bind_tools(\n",
    "            [To_Loan_tool_1])\n",
    "\n",
    "        generation = primary_assistant_runnable.invoke({\"messages\": messages, \"name\": name})\n",
    "        \n",
    "           \n",
    "        return {\n",
    "            \"messages\": generation\n",
    "        }\n",
    "\n",
    "    def tool_runnable(self):\n",
    "        llm = ChatOpenAI(model='gpt-3.5-turbo')\n",
    "\n",
    "\n",
    "        # llm = ChatGroq(model=\"llama3-70b-8192\", temperature=0)\n",
    "        loan_hotel_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\",\n",
    "                 \" You are a specialized assistant for calculating loan amount of a customer \"\n",
    "                 \" The primary assistant delegates work to you whenever the user needs help with calculating loan amount\"\n",
    "                 \"  When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n",
    "                 \" Once you have calculated loan amount delegate back to  main assistant.\"\n",
    "                 \"  Remember that a loan amount  isn't completed until after the relevant tool has successfully been used.\"\n",
    "                 \"  then CompleteOrEscalate the dialog to the host assistant.\"\n",
    "                 \"  Do not waste the user's time. Do not make up invalid tools or functions.\"\n",
    "                 \" You dont need first name of the customer that's it\"\n",
    "                 \"  then end the conversation by say such word as bye \"\n",
    "                 \" You just need first name to calculate the loan amount \"\n",
    "                 \" Remember to tell the user they will get 5% discount in their loan amount ,if they agree then that will\"\n",
    "                 \"be their loan amount ,if they disagree offer them 10 % discount in their loan amount only if they \"\n",
    "                 \"disagree in 5 % loan amount calculation\"\n",
    "                 \"Only 5% and 10% loan adjustment is possible beyond that not possible\"\n",
    "                 \" Name of the customer is {name}\"\n",
    "                 \" The loan tool will tell how much amount will the customer will pay this month\"\n",
    "                 \" If you have calculated the loan amount then use  CompleteOrEscalate function call /tool\"\n",
    "                 \" \\n\\nSome examples for which you should CompleteOrEscalate:\\n\"\"\"\n",
    "                 \"  - 'Loan amount calculated ',\"\n",
    "                 \"  -The loan amount for this month will be $16.67 for James\"\n",
    "                 \" tell him this will be the loan amount the user have pay for this month\"\n",
    "                 ),\n",
    "                (\"placeholder\", \"{messages}\")\n",
    "            ]\n",
    "        )\n",
    "        tool_1 = [monthly_payment]\n",
    "        loan_tool_runnable = loan_hotel_prompt | llm.bind_tools(tool_1 + [CompleteOrEscalate])\n",
    "        return loan_tool_runnable\n",
    "\n",
    "    def create_entry_node(self, assistant_name: str, new_dialog_state: str) -> Callable:\n",
    "        def entry_node(state: State) -> dict:\n",
    "            tool_call_id = state[\"messages\"][-1].tool_calls[0][\"id\"]\n",
    "            return {\n",
    "                \"messages\": [\n",
    "                    ToolMessage(\n",
    "                        content=f\"The assistant is now the {assistant_name}. Reflect on the above conversation between the host assistant and the user.\"\n",
    "                                f\" The user's intent is unsatisfied. Use the provided tools to assist the user. Remember, you are {assistant_name},\"\n",
    "                                \" remember to  invoked the appropriate tool for calculating loan adjustment\"\n",
    "                                \" If the user changes their mind or needs help for other tasks, call the CompleteOrEscalate function to let the primary host assistant take control.\"\n",
    "                                \" Do not mention who you are - just act as the proxy for the assistant.\",\n",
    "                        tool_call_id=tool_call_id,\n",
    "                    )\n",
    "                ],\n",
    "                \"dialog_state\": new_dialog_state,\n",
    "            }\n",
    "\n",
    "        return entry_node\n",
    "\n",
    "\n",
    "def route_to_tool(\n",
    "        state: State,\n",
    ") -> Literal[\n",
    "    \"tool_use\",\n",
    "    \"leave_skill\",\n",
    "    \"__end__\",\n",
    "]:\n",
    "    route = tools_condition(state)\n",
    "    if route == END:\n",
    "        return END\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    did_cancel = any(tc[\"name\"] == CompleteOrEscalate.__name__ for tc in tool_calls)\n",
    "    if did_cancel:\n",
    "        return \"leave_skill\"\n",
    "    tool_names = [t.name for t in tools]\n",
    "    if all(tc[\"name\"] in tool_names for tc in tool_calls):\n",
    "        return \"tool_use\"\n",
    "\n",
    "\n",
    "def pop_dialog_state(state: State) -> dict:\n",
    "    \"\"\"Pop the dialog stack and return to the main assistant.\n",
    "\n",
    "    This lets the full graph explicitly track the dialog flow and delegate control\n",
    "    to specific sub-graphs.\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "    if state[\"messages\"][-1].tool_calls:\n",
    "        # Note: Doesn't currently handle the edge case where the llm performs parallel tool calls\n",
    "        messages.append(\n",
    "            ToolMessage(\n",
    "                content=\"Resuming dialog with the host assistant. Please reflect on the past conversation and assist the user as needed.\",\n",
    "                tool_call_id=state[\"messages\"][-1].tool_calls[0][\"id\"],\n",
    "            )\n",
    "        )\n",
    "    return {\n",
    "        \"dialog_state\": \"pop\",\n",
    "        \"messages\": messages,\n",
    "    }\n",
    "\n",
    "\n",
    "def route_primary_assistant(\n",
    "        state: State,\n",
    ") -> Literal[\n",
    "    \"enter_loan_tool\",\n",
    "    \"__end__\",\n",
    "]:\n",
    "    route = tools_condition(state)\n",
    "    if route == END:\n",
    "        return END\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    if tool_calls:\n",
    "        if tool_calls[0][\"name\"] == To_Loan_tool_1.__name__:\n",
    "            return \"enter_loan_tool\"\n",
    "    raise ValueError(\"Invalid route\")\n",
    "\n",
    "\n",
    "def route_to_workflow(\n",
    "        state: State,\n",
    ") -> Literal[\n",
    "    \"primary_assistant\",\n",
    "    \"update_loan\"\n",
    "]:\n",
    "    dialog_state = state.get(\"dialog_state\")\n",
    "    if not dialog_state:\n",
    "        return \"primary_assistant\"\n",
    "    return dialog_state[-1]\n",
    "\n",
    "\n",
    "def handle_tool_error(state) -> dict:\n",
    "    error = state.get(\"error\")\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=f\"Error: {repr(error)}\\n please fix your mistakes.\",\n",
    "                tool_call_id=tc[\"id\"],\n",
    "            )\n",
    "            for tc in tool_calls\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def create_tool_node_with_fallback(tool):\n",
    "    return ToolNode(tool).with_fallbacks(\n",
    "        [RunnableLambda(handle_tool_error)], exception_key=\"error\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5f5fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76ff692e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "cal_tools = [monthly_payment]\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "\n",
    "class WorkFlow:\n",
    "    def __init__(self):\n",
    "        nodes = Nodes()\n",
    "        workflow = StateGraph(State)\n",
    "        # ADDING NODES\n",
    "        # workflow.add_node(\"user_profile\", nodes.customer_profile_summarizer)\n",
    "        workflow.set_entry_point(\"primary_assistant\")\n",
    "        workflow.add_node(\"primary_assistant\", nodes.primary_assistant)\n",
    "        workflow.add_node(\"enter_loan_tool\", nodes.create_entry_node(\"Loan_calculator_assistant\", \"update_loan\"))\n",
    "        workflow.add_node(\"update_loan\", Assistant(nodes.tool_runnable()))\n",
    "        workflow.add_edge(\"enter_loan_tool\", \"update_loan\")\n",
    "        workflow.add_node(\"tool_use\", create_tool_node_with_fallback(cal_tools))\n",
    "        workflow.add_edge(\"tool_use\", \"update_loan\")\n",
    "        workflow.add_conditional_edges(\n",
    "            \"update_loan\",\n",
    "            route_to_tool,\n",
    "            {\n",
    "                \"leave_skill\": \"leave_skill\",\n",
    "                END: END,\n",
    "                \"tool_use\": \"tool_use\"\n",
    "            }\n",
    "        )\n",
    "        workflow.add_node(\"leave_skill\", pop_dialog_state)\n",
    "        workflow.add_edge(\"leave_skill\", \"primary_assistant\")\n",
    "        workflow.add_conditional_edges(\n",
    "            \"primary_assistant\",\n",
    "            route_primary_assistant,\n",
    "            {\n",
    "                \"enter_loan_tool\": \"enter_loan_tool\",\n",
    "                END: END\n",
    "            }\n",
    "        )\n",
    "        # workflow.add_conditional_edges(\"user_profile\", route_to_workflow)\n",
    "\n",
    "        self.app = workflow.compile(checkpointer=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa4d07a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently in:  update_loan\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "yeah a loan adjustment would be great\n",
      "Currently in:  update_loan\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Great! Your loan amount for this month is $299.997. Thank you for your cooperation. If you need any further assistance or have any more questions, feel free to let me know.\n",
      "Currently in:  update_loan\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "seems good to me\n",
      "Currently in:  update_loan\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Great! I'm glad the loan adjustment meets your satisfaction. If you have any more questions or need further assistance in the future, feel free to reach out. Have a wonderful day!\n",
      "Currently in:  update_loan\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Still the adjustment seem to be more steep\n",
      "Currently in:  update_loan\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I apologize for the inconvenience. Let me try to assist you with the loan adjustment again. Please give me a moment to recalculate the revised loan amount for you. Thank you for your patience.\n",
      "Tool Calls:\n",
      "  To_Loan_tool_1 (call_rZqTNlKUxgIBRFlSjQERB9wH)\n",
      " Call ID: call_rZqTNlKUxgIBRFlSjQERB9wH\n",
      "  Args:\n",
      "    rate: {'name': 'James', 'rate': 10}\n",
      "Currently in:  update_loan\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "The assistant is now the Loan_calculator_assistant. Reflect on the above conversation between the host assistant and the user. The user's intent is unsatisfied. Use the provided tools to assist the user. Remember, you are Loan_calculator_assistant, remember to  invoked the appropriate tool for calculating loan adjustment If the user changes their mind or needs help for other tasks, call the CompleteOrEscalate function to let the primary host assistant take control. Do not mention who you are - just act as the proxy for the assistant.\n",
      "Currently in:  update_loan\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  monthly_payment (call_7YEqAgv9qHMbjo5AAXIfQrel)\n",
      " Call ID: call_7YEqAgv9qHMbjo5AAXIfQrel\n",
      "  Args:\n",
      "    name: James\n",
      "    rate: 10\n",
      "Currently in:  update_loan\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: monthly_payment\n",
      "\n",
      "This will be the last 299.997  payment for the customer James\n",
      "Currently in:  update_loan\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The loan amount for this month will be $299.997 for James. If you agree, this will be the amount you have to pay for this month. If you disagree, I can offer a 10% discount on the loan amount. Would you like to proceed with the calculated amount or avail the 10% discount?\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import uuid\n",
    "\n",
    "from groq import Groq\n",
    "from openai import OpenAI\n",
    "from pprint import pprint\n",
    "\n",
    "import streamlit as st\n",
    "import soundfile as sf\n",
    "from src.tools import _print_event\n",
    "\n",
    "\n",
    "\n",
    "thread_id = str(uuid.uuid4())\n",
    "_printed = set()\n",
    "\n",
    "app = WorkFlow().app\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # Checkpoints are accessed by thread_id\n",
    "        \"thread_id\": \"2ep-\",\n",
    "\n",
    "    }\n",
    "}\n",
    "_printed = set()\n",
    "\n",
    "qn = [\n",
    "\"yeah a loan adjustment would be great\",\n",
    "\"seems good to me\",\n",
    "\"Still the adjustment seem to be more steep\"\n",
    "]\n",
    "for question in qn:\n",
    "    events = app.stream(\n",
    "        {\"messages\": (\"user\", question), \"name\": \"James\"}, config, stream_mode=\"values\"\n",
    "    )\n",
    "    for event in events:\n",
    "        _print_event(event, _printed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5cfd84f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'loan_agent-response-v-reference-48125163' at:\n",
      "https://smith.langchain.com/o/f9ef5077-51b8-5888-95bb-c2ca8656d3f7/datasets/041cf538-e56f-4a3d-b789-4e9c5accd213/compare?selectedSessions=a80de0ec-facf-450c-84c2-867770b85519\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8acb5fb34fcf43cd84c28063d5528aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-KxEBT8NMpYrycBgaMKbqzq7t on tokens per min (TPM): Limit 60000, Used 59585, Requested 3576. Please try again in 3.161s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-KxEBT8NMpYrycBgaMKbqzq7t on tokens per min (TPM): Limit 60000, Used 59577, Requested 3595. Please try again in 3.172s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-KxEBT8NMpYrycBgaMKbqzq7t on tokens per min (TPM): Limit 60000, Used 59558, Requested 3603. Please try again in 3.161s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-KxEBT8NMpYrycBgaMKbqzq7t on tokens per min (TPM): Limit 60000, Used 59544, Requested 3610. Please try again in 3.154s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-KxEBT8NMpYrycBgaMKbqzq7t on tokens per min (TPM): Limit 60000, Used 59517, Requested 3607. Please try again in 3.124s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-KxEBT8NMpYrycBgaMKbqzq7t on tokens per min (TPM): Limit 60000, Used 59486, Requested 3622. Please try again in 3.108s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-KxEBT8NMpYrycBgaMKbqzq7t on tokens per min (TPM): Limit 60000, Used 56803, Requested 3603. Please try again in 406ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-KxEBT8NMpYrycBgaMKbqzq7t on tokens per min (TPM): Limit 60000, Used 56813, Requested 3576. Please try again in 389ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 2328765e-3d87-467a-8dae-0e216177802b: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\91639\\AppData\\Local\\Temp\\ipykernel_19112\\3242906969.py\", line 30, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "                 ~~~~~~~~~~~^^^^^^^^^^^^\n",
      "KeyError: 'response'\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 9c633353-5d9c-461a-9b2b-deb4da48f458: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\91639\\AppData\\Local\\Temp\\ipykernel_19112\\3242906969.py\", line 30, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "                 ~~~~~~~~~~~^^^^^^^^^^^^\n",
      "KeyError: 'response'\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 2663211f-7e8a-47b4-ad63-63038eb27120: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\91639\\AppData\\Local\\Temp\\ipykernel_19112\\3242906969.py\", line 30, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "                 ~~~~~~~~~~~^^^^^^^^^^^^\n",
      "KeyError: 'response'\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run b89bea87-9b07-4154-9005-5028f0c939be: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\91639\\AppData\\Local\\Temp\\ipykernel_19112\\3242906969.py\", line 30, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "                 ~~~~~~~~~~~^^^^^^^^^^^^\n",
      "KeyError: 'response'\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run f87e73b8-14bd-4d90-93d5-791fcaf26d95: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\91639\\AppData\\Local\\Temp\\ipykernel_19112\\3242906969.py\", line 30, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "                 ~~~~~~~~~~~^^^^^^^^^^^^\n",
      "KeyError: 'response'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 64389822-750d-4f7d-979f-e0beca0e8383: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\91639\\AppData\\Local\\Temp\\ipykernel_19112\\3242906969.py\", line 30, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "                 ~~~~~~~~~~~^^^^^^^^^^^^\n",
      "KeyError: 'response'\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run d0122447-a6ca-4d80-8818-3cc87189d9a7: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\91639\\AppData\\Local\\Temp\\ipykernel_19112\\3242906969.py\", line 30, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "                 ~~~~~~~~~~~^^^^^^^^^^^^\n",
      "KeyError: 'response'\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run e9f1ecfa-ee89-4821-8afb-21c7236412e1: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Users\\91639\\Desktop\\Python\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\91639\\AppData\\Local\\Temp\\ipykernel_19112\\3242906969.py\", line 30, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "                 ~~~~~~~~~~~^^^^^^^^^^^^\n",
      "KeyError: 'response'\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "app = WorkFlow().app\n",
    "\n",
    "client = Client()\n",
    "\n",
    "\n",
    "def predict_loan_agent_answer(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    msg = {\"messages\": (\"user\", example[\"input\"]),\"name\":\"James\"}\n",
    "    messages = app.invoke(msg, config)\n",
    "    return {\"response\": messages['messages'][-1].content}\n",
    "\n",
    "\n",
    "dataset_name = \"Loan agent response\"\n",
    "# Grade prompt\n",
    "grade_prompt_answer_accuracy = prompt = hub.pull(\"langchain-ai/rag-answer-vs-reference\")\n",
    "\n",
    "\n",
    "def answer_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for RAG answer accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    # Get question, ground truth answer, RAG chain answer\n",
    "    input_question = example.inputs[\"input\"]\n",
    "    reference = example.outputs[\"output\"]\n",
    "    prediction = run.outputs[\"response\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatGroq(model=\"llama3-70b-8192\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_answer_accuracy | llm\n",
    "\n",
    "    # Run evaluator\n",
    "    score = answer_grader.invoke({\"question\": input_question,\n",
    "                                  \"correct_answer\": reference,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"answer_v_reference_score\", \"score\": score}\n",
    "\n",
    "\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "experiment_prefix = \"loan_agent\"\n",
    "metadata = \"ABC bank\"\n",
    "experiment_results = evaluate(\n",
    "    predict_loan_agent_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_evaluator],\n",
    "    experiment_prefix=experiment_prefix + \"-response-v-reference\",\n",
    "    num_repetitions=3,\n",
    "    metadata={\"version\": metadata})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ecb0cd2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "variable messages should be a list of base messages, got hi ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m c\u001b[38;5;241m=\u001b[39mNodes()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprimary_assistant\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhi \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjames\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[29], line 230\u001b[0m, in \u001b[0;36mNodes.primary_assistant\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# llm = ChatGroq(model=\"llama3-70b-8192\", temperature=0)\u001b[39;00m\n\u001b[0;32m    227\u001b[0m primary_assistant_runnable \u001b[38;5;241m=\u001b[39m primary_assistant_prompt \u001b[38;5;241m|\u001b[39m llm\u001b[38;5;241m.\u001b[39mbind_tools(\n\u001b[0;32m    228\u001b[0m     [To_Loan_tool_1])\n\u001b[1;32m--> 230\u001b[0m generation \u001b[38;5;241m=\u001b[39m \u001b[43mprimary_assistant_runnable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: generation\n\u001b[0;32m    235\u001b[0m }\n",
      "File \u001b[1;32m~\\Desktop\\Python\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2499\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   2497\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2498\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[1;32m-> 2499\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2500\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2501\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   2502\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2503\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2504\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2505\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2506\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   2507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\Desktop\\Python\\Lib\\site-packages\\langchain_core\\prompts\\base.py:128\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags:\n\u001b[0;32m    127\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags\n\u001b[1;32m--> 128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_prompt_with_error_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Python\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1626\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1622\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m   1623\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, child_config)\n\u001b[0;32m   1624\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   1625\u001b[0m         Output,\n\u001b[1;32m-> 1626\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1628\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1629\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1630\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1631\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1632\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1633\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   1634\u001b[0m     )\n\u001b[0;32m   1635\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1636\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32m~\\Desktop\\Python\\Lib\\site-packages\\langchain_core\\runnables\\config.py:347\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    346\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Python\\Lib\\site-packages\\langchain_core\\prompts\\base.py:112\u001b[0m, in \u001b[0;36mBasePromptTemplate._format_prompt_with_error_handling\u001b[1;34m(self, inner_input)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_format_prompt_with_error_handling\u001b[39m(\u001b[38;5;28mself\u001b[39m, inner_input: Dict) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[0;32m    111\u001b[0m     _inner_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_input(inner_input)\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_inner_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Python\\Lib\\site-packages\\langchain_core\\prompts\\chat.py:665\u001b[0m, in \u001b[0;36mBaseChatPromptTemplate.format_prompt\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_prompt\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[0;32m    657\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;124;03m    Format prompt. Should return a PromptValue.\u001b[39;00m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;124;03m        PromptValue.\u001b[39;00m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 665\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    666\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatPromptValue(messages\u001b[38;5;241m=\u001b[39mmessages)\n",
      "File \u001b[1;32m~\\Desktop\\Python\\Lib\\site-packages\\langchain_core\\prompts\\chat.py:1008\u001b[0m, in \u001b[0;36mChatPromptTemplate.format_messages\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   1004\u001b[0m     result\u001b[38;5;241m.\u001b[39mextend([message_template])\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m   1006\u001b[0m     message_template, (BaseMessagePromptTemplate, BaseChatPromptTemplate)\n\u001b[0;32m   1007\u001b[0m ):\n\u001b[1;32m-> 1008\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[43mmessage_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1009\u001b[0m     result\u001b[38;5;241m.\u001b[39mextend(message)\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Desktop\\Python\\Lib\\site-packages\\langchain_core\\prompts\\chat.py:196\u001b[0m, in \u001b[0;36mMessagesPlaceholder.format_messages\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    190\u001b[0m value \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    191\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariable_name, [])\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptional\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m kwargs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariable_name]\n\u001b[0;32m    194\u001b[0m )\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should be a list of base messages, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m     )\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_messages(value)\n",
      "\u001b[1;31mValueError\u001b[0m: variable messages should be a list of base messages, got hi "
     ]
    }
   ],
   "source": [
    "c=Nodes()\n",
    "c.primary_assistant({\"messages\": \"hi\", \"name\": \"james\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7971bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "I have calculated two loan amounts for you, James, with different discounts. \n",
    "- With a 5% discount, the new monthly payment amount will be $316.6635.\n",
    "- With a 10% discount, the new monthly payment amount will be $299.997."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
